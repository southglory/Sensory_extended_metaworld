{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Quick Start\n",
    "- conda create -n {name} python=3.9\n",
    "- conda activate {name}\n",
    "- cd {metaworld root}\n",
    "- pip install -e.\n",
    "- conda install -c anaconda ipykernel\n",
    "- conda install -c conda-forge opencv\n",
    "- conda install -c anaconda pytest\n",
    "- conda install -c anaconda scipy\n",
    "- pip uninstall pygame\n",
    "- pip isntall pygame\n",
    "- conda install -c conda-forge matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For preventing some error message\n",
    "import gym\n",
    "gym.logger.set_level(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from tests.metaworld.envs.mujoco.sawyer_xyz.test_scripted_policies import ALL_ENVS, test_cases_latest_nonoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libGLEW.so:/usr/lib/nvidia/libGL.so\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/nvidia\n",
    "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/nvidia-510\n",
    "!export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libGLEW.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ssd_2/ssd2048/papers/Sensory_extended_metaworld/metaworld/scripts'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_trajectory_generator(env, policy, act_noise_pct, res=(640, 480), camera='corner1', depth = True):\n",
    "    action_space_ptp = env.action_space.high - env.action_space.low\n",
    "    env.reset()\n",
    "    env.reset_model()\n",
    "    o = env.reset()\n",
    "    \n",
    "    for _ in range(env.max_path_length):\n",
    "        a = policy.get_action(o)\n",
    "        a = np.random.normal(a, act_noise_pct * action_space_ptp)\n",
    "\n",
    "        o, r, done, info = env.step(a)\n",
    "        # Camera is one of ['corner', 'topview', 'behindGripper', ...]\n",
    "        # pose is like [x,y,z], z direction vector is normal to table surface.\n",
    "        # obj_pos, hand_pos, goal, obj_pos2, obj_quat2 =  env.obj_init_pos, env.hand_init_pos, env.goal, \\\n",
    "        # env.data.site_xpos[env.model.site_name2id('RoundNut-8')], env.sim.data.get_body_xquat('RoundNut')\n",
    "        # obj_angle, obj_pos, hand_pos, goal, obj_pos2, obj_quat2 =  env._get_pos_objects() , env._get_obs_dict(), env.hand_init_pos, env.goal, \\\n",
    "        # env.data.site_xpos[env.model.site_name2id('RoundNut-8')], env.sim.data.get_body_xquat('RoundNut')\n",
    "        # peg_pos = env._target_pos - np.array([0., 0., 0.05])\n",
    "        \n",
    "        img, d = env.sim.render(*res, mode='offscreen', camera_name=camera, depth = depth)\n",
    "        # print(depth)\n",
    "        #\"obj_angle\": obj_angle, \n",
    "        yield r, done, info, img, d, {}\n",
    "        # , {\"obj_pos\": obj_pos, \"hand_pos\": hand_pos, \"goal\": goal, \"obj_pos2\": obj_pos2, \"obj_quat2\": obj_quat2, \"peg_pos\": peg_pos}\n",
    "        \n",
    "def write_for_img(tag, img):\n",
    "    if not os.path.exists('latentfusion_inputs'):\n",
    "        os.mkdir('latentfusion_inputs')\n",
    "    name = f'latentfusion_inputs/{tag}.png'\n",
    "    return cv2.imwrite(name, img)\n",
    "\n",
    "def write_log_text(tag, pose, written):\n",
    "    if not written:\n",
    "        import time\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        name = f'latentfusion_inputs/{tag}.txt'\n",
    "        with open(name, \"a\") as file:\n",
    "            file.write(timestr+\":\\t\")\n",
    "            for key in pose:\n",
    "                file.write(key+\":\"+str(pose[key])+\"\\t\")\n",
    "            file.write(\"\\n\")\n",
    "            file.close()\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = (640, 480)\n",
    "camera = ['topview', 'corner1', 'corner2', 'corner3', 'behindGripper']\n",
    "flip=True # if True, flips output image 180 degrees\n",
    "\n",
    "config = [\n",
    "    # env, action noise pct, cycles, quit on success\n",
    "    ('button-press-topdown-v2', np.zeros(4), 1, True),\n",
    "]\n",
    "pose_log_written = False\n",
    "for camera in camera:\n",
    "    if camera in ['corner1', 'corner2', 'corner3']:\n",
    "        flip = False\n",
    "    else:\n",
    "        flip = True\n",
    "    for env, noise, cycles, quit_on_success in config:\n",
    "        tag = env + '-noise-' + np.array2string(noise, precision=2, separator=',', suppress_small=True)\\\n",
    "            + '-cycles-'+ str(cycles) +'-camera-'+ camera\n",
    "\n",
    "        policy = functools.reduce(lambda a,b : a if a[0] == env else b, test_cases_latest_nonoise)[1]\n",
    "        env = ALL_ENVS[env]()\n",
    "        env._partially_observable = False\n",
    "        env._freeze_rand_vec = False\n",
    "        env._set_task_called = True\n",
    "        for _ in range(cycles):\n",
    "            for r, done, info, img, depth, pose in my_trajectory_generator(env, policy, noise, resolution, camera, depth = True):\n",
    "                if flip: img = cv2.rotate(img, cv2.ROTATE_180); depth = cv2.rotate(depth, cv2.ROTATE_180)\n",
    "                # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                write_for_img(tag, img)\n",
    "                \n",
    "                # print(depth)\n",
    "                depth = (np.max(depth)-depth) / (np.max(depth) - np.min(depth))\n",
    "                # depth = (depth-np.min(depth)) / (np.max(depth) - np.min(depth))\n",
    "                depth = np.asarray(depth * 255, dtype=np.uint8)\n",
    "                # print(np.shape(depth))\n",
    "                # print(depth)\n",
    "                write_for_img(tag+'_depth', depth)\n",
    "                \n",
    "                # print(pose)\n",
    "                pose_log_written = write_log_text(\"pose\", pose, pose_log_written)\n",
    "                \n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is example log of \"latentfusion_inputs/pose.txt\".\n",
    "- 20220602-000011:\tobj_angle:0.3\tobj_pos:[0.         0.60000002 0.02      ]\thand_pos:[0.  0.6 0.2]\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_for_img_CH(tag, img):\n",
    "    if not os.path.exists('latentfusion_inputs/CH'):\n",
    "        os.mkdir('latentfusion_inputs/CH')\n",
    "    name = f'latentfusion_inputs/CH/{tag}.png'\n",
    "    return cv2.imwrite(name, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'write_for_img_CH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/ssd_2/ssd2048/papers/Sensory_extended_metaworld/metaworld/scripts/scripted_policy_movies_ours_new.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/ssd_2/ssd2048/papers/Sensory_extended_metaworld/metaworld/scripts/scripted_policy_movies_ours_new.ipynb#ch0000010?line=41'>42</a>\u001b[0m tag \u001b[39m=\u001b[39m env_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m-noise-\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39marray2string(noise, precision\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, separator\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, suppress_small\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\\\n\u001b[1;32m     <a href='vscode-notebook-cell:/ssd_2/ssd2048/papers/Sensory_extended_metaworld/metaworld/scripts/scripted_policy_movies_ours_new.ipynb#ch0000010?line=42'>43</a>\u001b[0m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m-cycles-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i) \u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m-camera-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m camera  \n\u001b[1;32m     <a href='vscode-notebook-cell:/ssd_2/ssd2048/papers/Sensory_extended_metaworld/metaworld/scripts/scripted_policy_movies_ours_new.ipynb#ch0000010?line=43'>44</a>\u001b[0m \u001b[39m# RGB(480, 640, 3) img, what you wanted.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/ssd_2/ssd2048/papers/Sensory_extended_metaworld/metaworld/scripts/scripted_policy_movies_ours_new.ipynb#ch0000010?line=44'>45</a>\u001b[0m write_for_img_CH(tag, img)\n\u001b[1;32m     <a href='vscode-notebook-cell:/ssd_2/ssd2048/papers/Sensory_extended_metaworld/metaworld/scripts/scripted_policy_movies_ours_new.ipynb#ch0000010?line=45'>46</a>\u001b[0m \u001b[39m# Depth(480, 640, 1) img\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/ssd_2/ssd2048/papers/Sensory_extended_metaworld/metaworld/scripts/scripted_policy_movies_ours_new.ipynb#ch0000010?line=46'>47</a>\u001b[0m write_for_img_CH(tag\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_depth\u001b[39m\u001b[39m'\u001b[39m, depth)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'write_for_img_CH' is not defined"
     ]
    }
   ],
   "source": [
    "import metaworld\n",
    "import random\n",
    "\n",
    "ml1 = metaworld.ML1('button-press-topdown-v2')\n",
    "\n",
    "env = ml1.train_classes['button-press-topdown-v2']()\n",
    "task = random.choice(ml1.train_tasks)\n",
    "env.set_task(task)\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "resolution = (640, 480)\n",
    "camera = ['topview', 'corner1', 'corner2', 'corner3']\n",
    "flip=True # if True, flips output image 180 degrees\n",
    "\n",
    "config = [\n",
    "    # env, action noise pct, cycles, quit on success\n",
    "    ('button-press-topdown-v2', np.zeros(4), 3, True),\n",
    "]\n",
    "\n",
    "\n",
    "for camera in camera:\n",
    "    if camera in ['corner1', 'corner2', 'corner3']:\n",
    "        flip = False\n",
    "    else:\n",
    "        flip = True\n",
    "    for env_name, noise, cycles, quit_on_success in config:  \n",
    "        for i in range(cycles):\n",
    "            a = env.action_space.sample()\n",
    "            obs, reward, done, info = env.step(a)\n",
    "            res=(640, 480)\n",
    "            img, depth = env.sim.render(*res, mode='offscreen', camera_name=camera, depth = True)\n",
    "            if flip: img = cv2.rotate(img, cv2.ROTATE_180); depth = cv2.rotate(depth, cv2.ROTATE_180)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            depth = (np.max(depth)-depth) / (np.max(depth) - np.min(depth))\n",
    "            # depth = (depth-np.min(depth)) / (np.max(depth) - np.min(depth))\n",
    "            depth = np.asarray(depth * 255, dtype=np.uint8)\n",
    "            depth2 = np.expand_dims(depth, axis=2)\n",
    "            rgbd = np.concatenate((img, depth2), axis=2)\n",
    "            combined_img = np.tile(depth2, (1, 1, 3)) + img\n",
    "            print(np.shape(combined_img))\n",
    "            tag = env_name + '-noise-' + np.array2string(noise, precision=2, separator=',', suppress_small=True)\\\n",
    "            + '-cycles-'+ str(i) +'-camera-'+ camera  \n",
    "            # RGB(480, 640, 3) img, what you wanted.\n",
    "            write_for_img_CH(tag, img)\n",
    "            # Depth(480, 640, 1) img\n",
    "            write_for_img_CH(tag+'_depth', depth)\n",
    "            # RGBD(480, 640, 4) img\n",
    "            write_for_img_CH(tag+'_rgbd', rgbd)\n",
    "            # combied(480, 640, 3) img which add duplicated depth value into each rgb channel..\n",
    "            write_for_img_CH(tag+'_combined', combined_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c863dab3e473e6a95fb3bd2f60bce5515eaebce20b5d05d4566631180f0114b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('Testaa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
